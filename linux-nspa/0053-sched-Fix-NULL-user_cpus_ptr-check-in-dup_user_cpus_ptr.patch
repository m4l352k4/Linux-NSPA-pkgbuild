diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 8df51b08bb38..6b259d9e127a 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2624,8 +2624,14 @@ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
 		      int node)
 {
+	cpumask_t *user_mask = NULL;
 	unsigned long flags;
 
+	/*
+	 * This check is racy and losing the race is a valid situation.
+	 * It is not worth the extra overhead of taking the pi_lock on
+	 * every fork/clone.
+	 */
 	if (!src->user_cpus_ptr)
 		return 0;
 
@@ -2633,10 +2639,23 @@ int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
 	if (!dst->user_cpus_ptr)
 		return -ENOMEM;
 
-	/* Use pi_lock to protect content of user_cpus_ptr */
+	/*
+	 * Use pi_lock to protect content of user_cpus_ptr
+	 *
+	 * Though unlikely, user_cpus_ptr can be reset to NULL by a concurrent
+	 * do_set_cpus_allowed(). When this happens, we need to clear
+	 * dst->user_cpus_ptr and free the allocated memory afterward.
+	 */
 	raw_spin_lock_irqsave(&src->pi_lock, flags);
-	cpumask_copy(dst->user_cpus_ptr, src->user_cpus_ptr);
+	if (src->user_cpus_ptr)
+		cpumask_copy(dst->user_cpus_ptr, src->user_cpus_ptr);
+	else
+		swap(dst->user_cpus_ptr, user_mask);
 	raw_spin_unlock_irqrestore(&src->pi_lock, flags);
+
+	if (unlikely(user_mask))
+		kfree(user_mask);
+
 	return 0;
 }
 
-- 
2.31.1

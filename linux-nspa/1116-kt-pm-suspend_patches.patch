From 246e82eacc04184b8b644e5c114af8c15b993f9f Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Sun, 17 Apr 2022 10:53:19 -0700
Subject: [PATCH] Revert "cpuidle: don't disable cpuidle when entering suspend"

This reverts commit 0b3cc77678b9500f240c9a550f74302d0f5760da.

The workflow of s2idle requires the cpuidle_resume() in dpm_noirq_end(),
otherwise cpuidle never gets re-enabled after exiting s2idle.

 --modified/rebased to apply over 5.18-nspa

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 drivers/base/power/main.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/base/power/main.c b/drivers/base/power/main.c
index 1b9d660bc4d1..e8676b395550 100644
--- a/drivers/base/power/main.c
+++ b/drivers/base/power/main.c
@@ -32,6 +32,7 @@
 #include <linux/suspend.h>
 #include <trace/events/power.h>
 #include <linux/cpufreq.h>
+#include <linux/cpuidle.h>
 #include <linux/devfreq.h>
 #include <linux/timer.h>
 
@@ -748,6 +749,7 @@ void dpm_resume_noirq(pm_message_t state
 
 	resume_device_irqs();
 	device_wakeup_disarm_wake_irqs();
+	cpuidle_resume();
 }
 
 /**
@@ -1350,6 +1352,7 @@ int dpm_suspend_noirq(pm_message_t state
 {
 	int ret;
 
+	cpuidle_pause();
 	device_wakeup_arm_wake_irqs();
 	suspend_device_irqs();

From 7175650880f66e197e55959173cf901e8a348ce1 Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Wed, 4 May 2022 09:54:17 -0700
Subject: [PATCH] kernel: Allow wakeup IRQs to cancel ongoing suspend

Wakeup IRQs are only "armed" to cancel suspend very late into the suspend
process, meaning that they cannot stop a suspend that's ongoing. This can
be particularly painful due to how long the freezer may spend trying to
freeze processes, during which time a wakeup IRQ cannot make the freezer
abort. Wakeup IRQs should be honored throughout the entire suspend process
rather than just at the end, so tweak the IRQ PM wakeup check to allow
unarmed wakeup IRQs to cancel suspend partway through.

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 kernel/irq/chip.c | 32 ++++++++++----------------------
 kernel/irq/pm.c   | 16 ++++++++++++++++
 2 files changed, 26 insertions(+), 22 deletions(-)

diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index ec8d50ba3139..d7d14e5b514c 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -501,23 +501,25 @@ static bool irq_check_poll(struct irq_de
 
 static bool irq_may_run(struct irq_desc *desc)
 {
-	unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
-
-	/*
-	 * If the interrupt is not in progress and is not an armed
-	 * wakeup interrupt, proceed.
-	 */
-	if (!irqd_has_set(&desc->irq_data, mask))
+	/* Proceed if the IRQ isn't in progress and isn't a wakeup interrupt */
+	if (!irqd_has_set(&desc->irq_data, IRQD_IRQ_INPROGRESS |
+			  IRQD_WAKEUP_ARMED | IRQD_WAKEUP_STATE))
 		return true;
 
 	/*
 	 * If the interrupt is an armed wakeup source, mark it pending
 	 * and suspended, disable it and notify the pm core about the
-	 * event.
+	 * event. If it's a wakeup interrupt and has yet to be armed
+	 * but suspend is in progress, do a wakeup to cancel suspend.
+	 * The IRQ will be allowed to run via the check right below.
 	 */
 	if (irq_pm_check_wakeup(desc))
 		return false;
 
+	/* Run the IRQ as usual if it's a wakeup source and isn't yet armed */
+	if (irqd_is_wakeup_set(&desc->irq_data))
+		return true;
+
 	/*
 	 * Handle a potential concurrent poll on a different core.
 	 */

diff --git a/kernel/irq/pm.c b/kernel/irq/pm.c
index 6bd9b58429cc..6dcef6e07c2e 100644
--- a/kernel/irq/pm.c
+++ b/kernel/irq/pm.c
@@ -14,6 +14,8 @@
 
 #include "internals.h"
 
+static bool pm_in_suspend;
+
 bool irq_pm_check_wakeup(struct irq_desc *desc)
 {
 	if (irqd_is_wakeup_armed(&desc->irq_data)) {
@@ -24,6 +26,8 @@ bool irq_pm_check_wakeup(struct irq_desc *desc)
 		pm_system_irq_wakeup(irq_desc_get_irq(desc));
 		return true;
 	}
+	if (pm_in_suspend && irqd_is_wakeup_set(&desc->irq_data))
+		pm_system_irq_wakeup(irq_desc_get_irq(desc));
 	return false;
 }
 
@@ -191,8 +195,20 @@ static struct syscore_ops irq_pm_syscore_ops = {
 	.resume		= irq_pm_syscore_resume,
 };
 
+static int irq_pm_notify(struct notifier_block *b, unsigned long event, void *p)
+{
+	pm_in_suspend = event == PM_SUSPEND_PREPARE;
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block irq_pm_notifier = {
+	.notifier_call = irq_pm_notify,
+	.priority = INT_MAX
+};
+
 static int __init irq_pm_init_ops(void)
 {
+	BUG_ON(register_pm_notifier(&irq_pm_notifier));
 	register_syscore_ops(&irq_pm_syscore_ops);
 	return 0;
 }
From cbdfb323f9cae2890aef636c6528a1315e584328 Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Fri, 4 Mar 2022 23:44:29 -0800
Subject: [PATCH] genirq: Fix spurious "handler enabled interrupts" warning on
 RT

The SDE IRQ handler routes interrupts itself using generic_handle_irq(),
which coughs up a warning on RT since the SDE IRQ is threaded and therefore
IRQs aren't disabled in the handler. Aside from the warning spam, this is
problematic because the warning unevenly disables IRQs thinking that IRQs
are supposed to be disabled to begin with, which results in all sorts of
mayhem. To fix it, check if IRQs were actually disabled prior to running a
given handler instead of assuming it by default.

 -- Modified/rebased for 5.18.2-nspa(-rt)
 
 we don't actually care about SDE IRQ handler, given that it doesn't exist
 in mainline - but we do care about "the warning unevenly disables IRQs..."
 part of the above comment.

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 kernel/irq/handle.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/kernel/irq/handle.c b/kernel/irq/handle.c
index 6c28b316a2cd..301c4769d07e 100644
--- a/kernel/irq/handle.c
+++ b/kernel/irq/handle.c
@@ -145,6 +145,7 @@ irqreturn_t __handle_irq_event_percpu(st
 	record_irq_time(desc);
 
 	for_each_action_of_desc(desc, action) {
+	        bool irqs_were_off = irqs_disabled();
 		irqreturn_t res;
 
 		/*
@@ -158,7 +159,8 @@ irqreturn_t __handle_irq_event_percpu(st
 		res = action->handler(irq, action->dev_id);
 		trace_irq_handler_exit(irq, action, res);
 
-		if (WARN_ONCE(!irqs_disabled(),"irq %u handler %pS enabled interrupts\n",
+		if (WARN_ONCE(irqs_were_off && !irqs_disabled(),
+			      "irq %u handler %pF enabled interrupts\n",
 			      irq, action->handler))
 			local_irq_disable();
 

From 72b078af900a9e959c95cb8ab546ffd18a088c07 Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Wed, 4 May 2022 09:14:14 -0700
Subject: [PATCH] PM / suspend: Clear wakeups before running PM callbacks

PM callbacks can be used as an indicator that the system is suspending, and
as such, a wakeup may be issued outside the notifier in order to cancel an
ongoing suspend. Clearing wakeups after running the PM callbacks can thus
cause wakeups to be missed. Fix it by simply clearing wakeups prior to
running PM callbacks.

 --modified/rebased for 5.18.2-nspa(-rt)

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 kernel/power/process.c | 1 -
 kernel/power/suspend.c | 1 +
 2 files changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/power/process.c b/kernel/power/process.c
index c9da2f439994..0a5e25a20864 100644
--- a/kernel/power/process.c
+++ b/kernel/power/process.c
@@ -132,7 +132,6 @@ int freeze_processes(void)
 	if (!pm_freezing)
 		static_branch_inc(&freezer_active);
 
-	pm_wakeup_clear(0);
 	pm_freezing = true;
 	error = try_to_freeze_tasks(true);
 	if (!error)
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 4732ea41fdfa..79b6b710bd3c 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -560,6 +560,7 @@ static int enter_state(suspend_state_t s
 	if (!mutex_trylock(&system_transition_mutex))
 		return -EBUSY;
 
+	pm_wakeup_clear(0);
 	if (state == PM_SUSPEND_TO_IDLE)
 		s2idle_begin();
 
From ffd7943b5013cb63143106063f451212f83ddfe2 Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Wed, 4 May 2022 09:28:34 -0700
Subject: [PATCH] PM / freezer: Abort suspend when there's a wakeup while
 freezing

Although try_to_freeze_tasks() stops when there's a wakeup, it doesn't
return an error when it successfully freezes everything it wants to freeze.
As a result, the suspend attempt can continue even after a wakeup is
issued. Although the wakeup will be eventually caught later in the suspend
process, kicking the can down the road is suboptimal; when there's a wakeup
detected, suspend should be immediately aborted by returning an error
instead. Make try_to_freeze_tasks() do just that, and also move the wakeup
check above the `todo` check so that we don't miss a wakeup from a process
that successfully froze.

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 kernel/power/process.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/power/process.c b/kernel/power/process.c
index 0a5e25a20864..856cb9b0bd97 100644
--- a/kernel/power/process.c
+++ b/kernel/power/process.c
@@ -66,14 +66,14 @@ static int try_to_freeze_tasks(bool user_only)
 			todo += wq_busy;
 		}
 
-		if (!todo || time_after(jiffies, end_time))
-			break;
-
 		if (pm_wakeup_pending()) {
 			wakeup = true;
 			break;
 		}
 
+		if (!todo || time_after(jiffies, end_time))
+			break;
+
 		/*
 		 * We need to retry, but first give the freezing tasks some
 		 * time to enter the refrigerator.  Start with an initial
@@ -114,7 +114,7 @@ static int try_to_freeze_tasks(bool user_only)
 			elapsed_msecs % 1000);
 	}
 
-	return todo ? -EBUSY : 0;
+	return todo || wakeup ? -EBUSY : 0;
 }
 
 /**

From 851f9989ed1607a2aa729a6b360e8a36ce088b79 Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Tue, 3 May 2022 23:32:26 -0700
Subject: [PATCH] PM / wakeup: Avoid excessive s2idle wake attempts in
 pm_system_wakeup()

Calling s2idle_wake() once after pm_abort_suspend is incremented is all
that's needed to wake up from s2idle. Avoid multiple unnecessary attempts
to wake from s2idle by only doing the wakeup when pm_abort_suspend hits 1.
The s2idle machinery already provides the synchronization needed to make
this safe.

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 drivers/base/power/wakeup.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/base/power/wakeup.c b/drivers/base/power/wakeup.c
index 31394e8fd481..cbb291daf822 100644
--- a/drivers/base/power/wakeup.c
+++ b/drivers/base/power/wakeup.c
@@ -955,8 +955,8 @@ bool pm_wakeup_pending(void)
 
 void pm_system_wakeup(void)
 {
-	atomic_inc(&pm_abort_suspend);
-	s2idle_wake();
+	if (atomic_inc_return_relaxed(&pm_abort_suspend) == 1)
+		s2idle_wake();
 }
 EXPORT_SYMBOL_GPL(pm_system_wakeup);
 

From 333ea93ca815491487007f1d712a574fe4f9d4c3 Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Sat, 30 Apr 2022 11:47:30 -0700
Subject: [PATCH] timekeeping: Keep the tick alive when CPUs cycle out of
 s2idle

When some CPUs cycle out of s2idle due to non-wakeup IRQs, it's possible
for them to run while the CPU responsible for jiffies updates remains idle.
This can delay the execution of timers indefinitely until the CPU managing
the jiffies updates finally wakes up, by which point everything could be
dead if enough time passes.

Fix it by handing off timekeeping duties when the timekeeping CPU enters
s2idle and freezes its tick. When all CPUs are in s2idle, the first one to
wake up for any reason (either from a wakeup IRQ or non-wakeup IRQ) will
assume responsibility for the timekeeping tick.

 --modified/rebased to apply over 5.18-nspa

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 kernel/time/tick-common.c | 21 ++++++++++++++++-----
 1 file changed, 16 insertions(+), 5 deletions(-)

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b9c077a160ed..d228058405b2 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -510,7 +510,7 @@ void tick_resume(void)
 #ifdef CONFIG_SUSPEND
 static DEFINE_RAW_SPINLOCK(tick_freeze_lock);
 static DEFINE_WAIT_OVERRIDE_MAP(tick_freeze_map, LD_WAIT_SLEEP);
-static unsigned int tick_freeze_depth;
+static unsigned long tick_frozen_mask;
 
 /**
  * tick_freeze - Suspend the local tick and (possibly) timekeeping.
@@ -523,10 +523,17 @@ static unsigned int tick_freeze_depth;
  */
 void tick_freeze(void)
 {
+	int cpu = smp_processor_id();
+
 	raw_spin_lock(&tick_freeze_lock);
 
-	tick_freeze_depth++;
-	if (tick_freeze_depth == num_online_cpus()) {
+	tick_frozen_mask |= BIT(cpu);
+	if (tick_do_timer_cpu == cpu) {
+		cpu = ffz(tick_frozen_mask);
+		tick_do_timer_cpu = (cpu < nr_cpu_ids) ? cpu :
+			TICK_DO_TIMER_NONE;
+	}
+	if (tick_frozen_mask == *cpumask_bits(cpu_online_mask)) {
 		trace_suspend_resume(TPS("timekeeping_freeze"),
 				     smp_processor_id(), true);
 		/*
@@ -563,9 +570,10 @@ void tick_freeze(void)
  */
 void tick_unfreeze(void)
 {
+	int cpu = smp_processor_id();
 	raw_spin_lock(&tick_freeze_lock);
 
-	if (tick_freeze_depth == num_online_cpus()) {
+	if (tick_frozen_mask == *cpumask_bits(cpu_online_mask)) {
 		/*
 		 * Similar to tick_freeze(). On resumption the first CPU may
 		 * acquire uncontended sleeping locks while other CPUs block on
@@ -583,8 +591,10 @@ void tick_unfreeze(void)
 		touch_softlockup_watchdog();
 		tick_resume_local();
 	}
+	if (tick_do_timer_cpu == TICK_DO_TIMER_NONE)
+		tick_do_timer_cpu = cpu;
 
-	tick_freeze_depth--;
+	tick_frozen_mask &= ~BIT(cpu);
 
 	raw_spin_unlock(&tick_freeze_lock);
 }
